{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Pure_Python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Package Import"
      ],
      "metadata": {
        "id": "G_6Hk7-eT2f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "upXpzeZiT-z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "c7EaxQp-UFhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "\n",
        "def merge_batches(num_to_load=1):\n",
        "    for i in range(num_to_load):\n",
        "        fileName = \"/home/amir/Documents/Machine Vision/HW#4/cifar-10-python/cifar-10-batches-py/data_batch_\" + str(i + 1)\n",
        "        data = unpickle(fileName)\n",
        "        if i == 0:\n",
        "            features = data[b'data']\n",
        "            labels = np.array(data[b'labels'])\n",
        "        else:\n",
        "            features = np.append(features, data[b'data'], axis=0)\n",
        "            labels = np.append(labels, data[b'labels'], axis=0)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "class Utility:\n",
        "    @classmethod\n",
        "    def sigmoid(cls, arr):\n",
        "        return 1 / (1 + np.exp(-arr))\n",
        "\n",
        "    @classmethod\n",
        "    def sigmoid_d(cls, arr):\n",
        "        return cls.sigmoid(arr) * (1 - cls.sigmoid(arr))\n",
        "    \n",
        "    @classmethod\n",
        "    def mean_square_loss(cls, y, y_hat):\n",
        "        return (la.norm(y - y_hat) ** 2) / y.shape[0]\n",
        "\n",
        "    @classmethod\n",
        "    def get_batch(cls, X, y, idx, batch_size=100):\n",
        "        return X[idx*batch_size:(idx+1)*batch_size], y[idx*batch_size:(idx+1)*batch_size]\n",
        "\n",
        "    @classmethod\n",
        "    def SigmoidCrossEntropyLoss(cls, a, y):\n",
        "        a = cls.softmax(a)\n",
        "        return np.sum(np.nan_to_num(-y * np.log(a) - (1-y) * np.log(1-a))) / (10*y.shape[0])\n",
        "\n",
        "    @classmethod\n",
        "    def one_hot_encode(cls, data, size=10):\n",
        "        one_hot = np.zeros((data.shape[0], size))\n",
        "        one_hot[np.arange(data.shape[0]), data] = 1\n",
        "        return one_hot"
      ],
      "metadata": {
        "id": "6_u2iZbAT-_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-layer Perceptron Network"
      ],
      "metadata": {
        "id": "Hha2CsauUUdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, x, y, hs=50, os=10):\n",
        "        self.hidden_size = hs\n",
        "        self.output_size = os\n",
        "        self.input = x\n",
        "        self.label = y\n",
        "        self.h = np.zeros(hs)\n",
        "        self.output = np.zeros(self.output_size)\n",
        "        self.W = np.random.randn(self.input.shape[1], self.hidden_size)\n",
        "        self.W_b = np.zeros((1, self.hidden_size))\n",
        "        self.U = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.U_b = np.zeros((1, self.output_size))\n",
        "\n",
        "    def feed_forward(self, x):\n",
        "        l1 = np.dot(x, self.W) + self.W_b\n",
        "        a1 = Utility.sigmoid(l1)\n",
        "        l2 = np.dot(a1, self.U) + self.U_b\n",
        "        a2 = Utility.sigmoid(l2)\n",
        "\n",
        "        return l1, a1, l2, a2\n",
        "\n",
        "    def backpropagate(self, x, y, batch_size):\n",
        "        r, h, z, o = self.feed_forward(x)\n",
        "        # loss = Utility.SigmoidCrossEntropyLoss(o, y)\n",
        "        loss = Utility.mean_square_loss(o, y)\n",
        "        loss_deriv = o - y\n",
        "        du = np.dot(h.T, loss_deriv * Utility.sigmoid_d(np.dot(h, self.U)))\n",
        "        dbu = np.dot(np.ones((1, batch_size)), loss_deriv * Utility.sigmoid_d(np.dot(h, self.U)))\n",
        "        dw = np.dot(x.T, np.dot(loss_deriv * Utility.sigmoid_d(np.dot(h, self.U)), self.U.T)\n",
        "                    * Utility.sigmoid_d(np.dot(x, self.W)))\n",
        "        dbw = np.dot(np.ones((1, batch_size)), np.dot(loss_deriv * Utility.sigmoid_d(np.dot(h, self.U)), self.U.T)\n",
        "                     * Utility.sigmoid_d(np.dot(x, self.W)))\n",
        "\n",
        "        return loss, du, dbu, dw, dbw\n",
        "\n",
        "    def train(self, gamma=0.9, alpha=0.1, batch_size=100, epoch=50):\n",
        "        losses = []\n",
        "        accs = []\n",
        "        iteration_num = int(len(self.input)/batch_size)\n",
        "        print(\"iteration number: \", iteration_num)\n",
        "        for e in range(epoch):\n",
        "            dw_prev, dbw_prev, du_prev, dbu_prev = 0, 0, 0, 0\n",
        "            for b in range(iteration_num-1):\n",
        "                batch_x, batch_y = Utility.get_batch(self.input, self.label, idx=b, batch_size=batch_size)\n",
        "                loss, du, dbu, dw, dbw = self.backpropagate(batch_x, batch_y, batch_size)\n",
        "                dw_prev = gamma*dw_prev + (1-gamma)*dw\n",
        "                dbw_prev = gamma*dbw_prev + (1-gamma)*dbw\n",
        "                du_prev = gamma*du_prev + (1-gamma)*du\n",
        "                dbu_prev = gamma*dbu_prev + (1-gamma)*dbu\n",
        "                self.W -= alpha*dw_prev\n",
        "                self.W_b -= alpha*dbw_prev\n",
        "                self.U -= alpha*du_prev\n",
        "                self.U_b -= alpha*dbu_prev\n",
        "\n",
        "            eval_batch_x, eval_batch_y = Utility.get_batch(self.input, self.label, idx=(iteration_num-1), batch_size=batch_size)\n",
        "            accs.append(self.evaluate(eval_batch_x, eval_batch_y))\n",
        "            losses.append(loss)\n",
        "            alpha *= 0.99\n",
        "            print(\"Epoch %d complete\\tLoss: %f\\n\" % (e, loss))\n",
        "        plt.plot(list(np.arange(0, epoch, 1)), losses, c='red')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "        plt.plot(list(np.arange(0, epoch, 1)), accs, c='green')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('training accuracy')\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        count = 0\n",
        "        r, h, z, o = self.feed_forward(x)\n",
        "        for output, _y in zip(o, y):\n",
        "            if np.argmax(output) == np.argmax(_y):\n",
        "                count += 1\n",
        "        print(\"Accuracy: %f\" % ((float(count) / x.shape[0]) * 100))\n",
        "        return float(count) / (x.shape[0]) * 100"
      ],
      "metadata": {
        "id": "_ZlFu4-1T_HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load & Prep"
      ],
      "metadata": {
        "id": "rYaf25dxUfer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = merge_batches(5)\n",
        "one_hot_label = Utility.one_hot_encode(label)\n",
        "data_prep = Normalizer().fit_transform(data)\n",
        "\n",
        "test_batch = unpickle('/home/amir/Documents/Machine Vision/HW#4/cifar-10-python/cifar-10-batches-py/test_batch')\n",
        "test_data = test_batch[b'data']\n",
        "test_data_prep = Normalizer().fit_transform(test_data)\n",
        "test_label = np.array(test_batch[b'labels'])\n",
        "one_hot_test_label = Utility.one_hot_encode(test_label)"
      ],
      "metadata": {
        "id": "x_E4aKc8UkaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Network"
      ],
      "metadata": {
        "id": "YSI7uaGrUbbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Network(x=data_prep, y=one_hot_label, hs=50)\n",
        "network.train()"
      ],
      "metadata": {
        "id": "s5orcq2lT_Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "GqHil4eEVH6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network.evaluate(test_data_prep, one_hot_test_label)"
      ],
      "metadata": {
        "id": "DmZ1O5aZT_OI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}